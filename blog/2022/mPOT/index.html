<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Improving Mini-batch Optimal Transport via Partial Transportation | Son Nguyen</title> <meta name="author" content="Son Nguyen"> <meta name="description" content="Son Nguyen's homepage "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%87%A9%E2%80%8B&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/https://github.com/nguyendangson/nguyendangson.github.io/assets/css/main.css"> <link rel="canonical" href="https://nguyendangson.github.io/https://github.com/nguyendangson/nguyendangson.github.io/blog/2022/mPOT/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/https://github.com/nguyendangson/nguyendangson.github.io/assets/js/theme.js"></script> <script src="/https://github.com/nguyendangson/nguyendangson.github.io/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://github.com/nguyendangson/nguyendangson.github.io/" rel="external nofollow noopener" target="_blank"><span class="font-weight-bold">Son </span> Nguyen</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/https://github.com/nguyendangson/nguyendangson.github.io/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/https://github.com/nguyendangson/nguyendangson.github.io/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/https://github.com/nguyendangson/nguyendangson.github.io/cv/">Resume</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Improving Mini-batch Optimal Transport via Partial Transportation</h1> <p class="post-meta">August 26, 2022</p> <p class="post-tags"> <a href="https://github.com/nguyendangson/nguyendangson.github.io/blog/2022" rel="external nofollow noopener" target="_blank"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="https://github.com/nguyendangson/nguyendangson.github.io/blog/tag/optimal-transport" rel="external nofollow noopener" target="_blank"> <i class="fas fa-hashtag fa-sm"></i> optimal-transport</a>   <a href="https://github.com/nguyendangson/nguyendangson.github.io/blog/tag/domain-adaptation" rel="external nofollow noopener" target="_blank"> <i class="fas fa-hashtag fa-sm"></i> domain-adaptation</a>   <a href="https://github.com/nguyendangson/nguyendangson.github.io/blog/tag/generative-models" rel="external nofollow noopener" target="_blank"> <i class="fas fa-hashtag fa-sm"></i> generative-models</a>     ·   <a href="https://github.com/nguyendangson/nguyendangson.github.io/blog/category/conference" rel="external nofollow noopener" target="_blank"> <i class="fas fa-tag fa-sm"></i> conference</a>   </p> </header> <article class="post-content"> <p><strong>Table of contents</strong></p> <ul> <li><a href="#introduction">Introduction</a></li> <li> <a href="#background">Background</a> <ul> <li><a href="#optimal-transport">Optimal Transport</a></li> <li><a href="#mini-batch-optimal-transport">Mini-batch Optimal Transport</a></li> </ul> </li> <li> <a href="#mini-batch-partial-optimal-transport">Mini-batch Partial Optimal Transport</a> <ul> <li><a href="#partial-optimal-transport">Partial Optimal Transport</a></li> <li><a href="#mini-batch-partial-optimal-transport-1">Mini-batch Partial Optimal Transport</a></li> <li><a href="#training-deep-networks-with-m-pot-loss">Training deep networks with m-POT loss</a></li> </ul> </li> <li><a href="#experiments">Experiments</a></li> <li><a href="#conclusion">Conclusion</a></li> <li><a href="#references">References</a></li> </ul> <h2 id="introduction">Introduction</h2> <p>The Optimal Transport (OT) theory has a long history in Applied mathematics and economics, and recently it has become a useful tool in machine learning applications such as deep generative models [1], domain adaptation [2], etc. Despite its popularity in ML, there are still major issues with using OT in large-scale datasets, those issues could be demonstrated in two following situations: “<strong>What if the number of supports is very large, for example millions?</strong>” and “<strong>What if the computation of optimal transport is repeated multiple times and has limited memory e.g., in deep learning?</strong>”. To deal with those problems, practitioners often replace the original large-scale computation of OT with cheaper computation on subsets of the whole dataset, which is widely referred to as mini-batch approaches [3, 4]. In particular, a min-batch is a sparse representation of the data. Hence, matching two sparse subsets of two datasets often leads to many wrong pairings between sample points of two distributions. That consequently results in the extremely inaccurate estimation of the transport plan and OT cost between distributions.</p> <h2 id="background">Background</h2> <h3 id="optimal-transport">Optimal Transport</h3> <p>Let \(\mu,\nu\) be discrete distributions of \(n\) supports, i.e. \(\mu := \frac{1}{n} \sum_{i=1}^n \delta_{x_i}\) and \(\nu := \frac{1}{n}\sum_{j=1}^{n} \delta_{y_j}\). Given distances between supports of two distributions as a matrix \(C\), the Optimal Transport (OT) problem reads:</p> \[\begin{equation} \text{OT}(\mu, \nu) = \min_{\pi \in \Pi(\mu, \nu)} \langle C,\pi \rangle \end{equation}\] <p>where \(\Pi(\mu, \nu) = \{ \pi \in \mathbb{R}_{+}^{n \times n} \mid \pi 1 = \mu, \pi^T 1 = \nu \}\) is the set of admissible transportation plans between \(\mu\) and \(\nu\).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/ot_example-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/ot_example-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/ot_example-1400.webp"></source> <img src="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/ot_example.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 1. An example of OT with \(n = 4\).</p> <h3 id="mini-batch-optimal-transport">Mini-batch Optimal Transport</h3> <p>The original \(n\) samples are divided into random mini-batches of size \(m \geq 1\), then an alternative solution to the original OT problem is formed by averaging these smaller OT solutions.</p> \[\begin{equation} \text{m-OT}^m(\mu, \nu) = \mathbb{E}_{(X, Y) \sim \overset{\otimes m}{\mu} \times \overset{\otimes m}{\nu}} [\text{OT}(P_X, P_Y)] \end{equation}\] <p>where \(\otimes\) denotes product measure, \(X = (x_1, \ldots, x_m)\) is the sampled mini-batch, and \(P_X = \frac{1}{m} \sum_{i=1}^m \delta_{x_i}\) is the corresponding discrete distribution. In practice, we can use subsampling to approximate the expectation, thus the empirical m-OT reads:</p> \[\begin{equation} \text{m-OT}_k^m(\mu, \nu) \approx \frac{1}{k} \sum_{i=1}^k [\text{OT}(P_{X_i}, P_{Y_i})] \end{equation}\] <p>where \((X_1, Y_1), \ldots, (X_k, Y_k) \sim \overset{\otimes m}{\mu} \otimes \overset{\otimes m}{\nu}\) and \(k\) is often set to 1 in previous works.</p> <p><strong>Misspecified matchings issue of m-OT:</strong> We can see that the optimal matchings at the mini-batch level in Figure 2 are different from the full-scale optimal transport. We call these pairs misspecified matchings since they are optimal on the local mini-batch scale but they are non-optimal on the global scale. The reason is that all samples in mini-batches are forced to be transported.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mot_example-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mot_example-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mot_example-1400.webp"></source> <img src="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mot_example.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 2. An example of m-OT with \(n = 4, m = 2\) and \(k = 2\).</p> <h2 id="mini-batch-partial-optimal-transport">Mini-batch Partial Optimal Transport</h2> <p>To alleviate misspecified matchings, we use partial optimal transport between mini-batches levels instead of optimal transport. The partial optimal transport is defined almost the same as optimal transport except it only allows a fraction of masses $s$ to be transported.</p> <h3 id="partial-optimal-transport">Partial Optimal Transport</h3> <p>Let \(\mu, \nu\) be discrete distributions of \(n\) supports. Given the fraction of masses \(0 \leq s \leq 1\), the Partial Optimal Transport (POT) problem reads:</p> \[\begin{equation} \text{POT}^s(\mu, \nu) = \min_{\pi \in \Pi_s(\mu, \nu)} \langle C,\pi \rangle \end{equation}\] <p>where \(\Pi_s(\mu, \nu) = \{ \pi \in \mathbb{R}_{+}^{n \times n} \mid \pi 1 \leq \mu, \pi^T 1 \leq \nu, 1^T \pi 1 = s \}\) is the set of admissible transportation plans between \(\mu\) and \(\nu\).</p> <h3 id="mini-batch-partial-optimal-transport-1">Mini-batch Partial Optimal Transport</h3> <p>Similar to m-OT, we define mini-batch POT (m-POT) which averages the partial optimal transport between mini-batches of size \(m \geq 1\) as:</p> \[\begin{equation} \text{m-POT}^{m,s}(\mu, \nu) = \mathbb{E}_{(X, Y) \sim \overset{\otimes m}{\mu} \otimes \overset{\otimes m}{\nu}} [\text{POT}^s(P_X, P_Y)] \end{equation}\] <p>where \(\otimes\) denotes product measure, \(X = (x_1, \ldots, x_m)\) is the sampled mini-batch, and \(P_X = \frac{1}{m} \sum_{i=1}^m \delta_{x_i}\) is the corresponding discrete distribution. In practice, we can use subsampling to approximate the expectation, thus the empirical m-POT reads:</p> \[\begin{equation} \text{m-POT}_k^{m, s}(\mu, \nu) \approx \frac{1}{k} \sum_{i=1}^k [\text{POT}^s(P_{X_i}, P_{Y_i})] \end{equation}\] <p>where \((X_1, Y_1), \ldots, (X_k, Y_k) \sim \overset{\otimes m}{\mu} \otimes \overset{\otimes m}{\nu}\) and \(k\) is often set to 1.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mpot_example-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mpot_example-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mpot_example-1400.webp"></source> <img src="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mpot_example.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 3. An example of m-POT with \(n = 4, m = 2, k = 2\) and \(s = \frac{1}{2}\).</p> <p>In Figure 3, POT gives the exact 2 matchings, alleviating the misspecified matchings issue.</p> <h3 id="training-deep-networks-with-m-pot-loss">Training deep networks with m-POT loss</h3> <p><strong>Parallel training:</strong> In the deep learning context, the supports are usually parameterized by neural networks. In addition, the gradient of neural networks is accumulated from each pair of mini-batches and only one pair of mini-batches are used in memory at a time. Since the computations on pairs of mini-batches are independent, we can use multiple devices to compute them.</p> <p><strong>Two-stage training:</strong> We also propose two-stage training for <em>domain adaptation</em>. We first find matchings between pairs of bigger mini-batches of size \(M\) on RAM. Then use it to obtain a mapping to create smaller mini-batches of size $m$ which are used on GPU for estimating the gradient of neural networks. This algorithm allows us to have better matchings since they are obtained from larger transportation problems.</p> <h2 id="experiments">Experiments</h2> <p>To validate the performance of the proposed methods, we carry out experiments on deep domain adaptation (DA). We observe that m-POT gives better-adapted classification accuracy on all datasets than the previous methods. Moreover, the two-stage training significantly improves the performance of DA on Office-Home and VisDA for both optimal transport and partial optimal transport.</p> <p style="text-align:center; font-style: italic"><img src="/assets/img/mPOT_DA_digits.png" alt="mPOT_DA_digits" style="display:block; margin-left:auto; margin-right:auto"> Table 1. DA results in classification accuracy on digits datasets (higher is better).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mPOT_DA_OfficeHome-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mPOT_DA_OfficeHome-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mPOT_DA_OfficeHome-1400.webp"></source> <img src="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mPOT_DA_OfficeHome.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p style="text-align:center; font-style: italic">Table 2. DA results in classification accuracy on the Office-Home dataset (higher is better).</p> <p style="text-align:center; font-style: italic"><img src="/assets/img/mPOT_DA_VisDA.png" alt="mPOT_DA_VisDA" style="display:block; margin-left:auto; margin-right:auto"> Table 3. DA results in classification accuracy on the VisDA dataset (higher is better).</p> <h2 id="conclusion">Conclusion</h2> <p>In this paper, we have introduced a novel mini-batch approach that is referred to as mini-batch partial optimal transport (m-POT). The new mini-batch approach is motivated by the issue of misspecified mappings in the conventional mini-batch optimal transport approach (m-OT). Via extensive experiment studies, we demonstrate that m-POT can perform better than current mini-batch methods including m-OT and m-UOT in domain adaptation applications. Furthermore, we propose the two-stage training approach for the deep DA that outperforms the conventional implementation. For further information, please refer to our work at <a href="https://proceedings.mlr.press/v162/nguyen22e/nguyen22e.pdf" rel="external nofollow noopener" target="_blank">https://proceedings.mlr.press/v162/nguyen22e/nguyen22e.pdf</a>.</p> <h2 id="references">References</h2> <p>[1] Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein generative adversarial networks. In International Conference on Machine Learning, pp. 214–223, 2017.</p> <p>[2] Courty, N., Flamary, R., Tuia, D., and Rakotomamonjy, A. Optimal transport for domain adaptation. IEEE transactions on pattern analysis and machine intelligence, 39(9):1853–1865, 2016.</p> <p>[3] Fatras, K., Zine, Y., Flamary, R., Gribonval, R., and Courty, N. Learning with minibatch Wasserstein: asymptotic and gradient properties. In AISTATS 2020-23nd International Conference on Artificial Intelligence and Statistics, volume 108, pp. 1–20, 2020.</p> <p>[4] Fatras, K., Zine, Y., Majewski, S., Flamary, R., Gribonval, R., and Courty, N. Minibatch optimal transport distances; analysis and applications. arXiv preprint arXiv:2101.01792, 2021b.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Son Nguyen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/https://github.com/nguyendangson/nguyendangson.github.io/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/https://github.com/nguyendangson/nguyendangson.github.io/assets/js/zoom.js"></script> <script defer src="/https://github.com/nguyendangson/nguyendangson.github.io/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>