<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://nguyendangson.github.io/https://github.com/nguyendangson/nguyendangson.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://nguyendangson.github.io/https://github.com/nguyendangson/nguyendangson.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-09T13:51:37+00:00</updated><id>https://nguyendangson.github.io/https://github.com/nguyendangson/nguyendangson.github.io/feed.xml</id><title type="html">blank</title><subtitle>Son Nguyen&apos;s homepage </subtitle><entry><title type="html">Self-Attention Amortized Distributional Projection Optimization for Sliced Wasserstein Point-Cloud Reconstruction</title><link href="https://nguyendangson.github.io/https://github.com/nguyendangson/nguyendangson.github.io/blog/2023/SADSW/" rel="alternate" type="text/html" title="Self-Attention Amortized Distributional Projection Optimization for Sliced Wasserstein Point-Cloud Reconstruction"/><published>2023-09-12T00:00:00+00:00</published><updated>2023-09-12T00:00:00+00:00</updated><id>https://nguyendangson.github.io/https://github.com/nguyendangson/nguyendangson.github.io/blog/2023/SADSW</id><content type="html" xml:base="https://nguyendangson.github.io/https://github.com/nguyendangson/nguyendangson.github.io/blog/2023/SADSW/"><![CDATA[<p><strong>Table of contents</strong></p> <ul> <li><a href="#introduction">Introduction</a></li> <li><a href="#background">Background</a> <ul> <li><a href="#optimal-transport">Optimal Transport</a></li> <li><a href="#max-sliced-wasserstein-distance">Max Sliced Wasserstein Distance</a></li> <li><a href="#amortized-projection-optimization">Amortized Projection Optimization</a></li> </ul> </li> <li><a href="#self-attention-amortized-distributional-projection-optimization">Self-Attention Amortized Distributional Projection Optimization</a> <ul> <li><a href="#amortized-distributional-projection-optimization">Amortized Distributional Projection Optimization</a></li> <li><a href="#self-attention-amortized-models">Self-Attention Amortized Models</a></li> </ul> </li> <li><a href="#experiments">Experiments</a></li> <li><a href="#conclusion">Conclusion</a></li> <li><a href="#references">References</a></li> </ul> <h2 id="introduction">Introduction</h2> <p>Based on the closed-form solution of Wasserstein distance in one dimension, Sliced Wasserstein (SW) has been utilized successfully in point-cloud representation learning [1, 2] due to its computational efficiency. However, the downside of SW is that it treats all projections the same due to the usage of a uniform distribution over projecting directions. Thus, max sliced Wasserstein (Max-SW) [3] distance was proposed as a solution for less discriminative projections of sliced Wasserstein (SW) distance. In applications that have various independent pairs of probability measures, amortized projection optimization [4] was introduced to predict the “max” projecting directions given two input measures instead of using projected gradient ascent multiple times. Despite being efficient, Max-SW and its amortized version cannot guarantee metricity property due to the sub-optimality of the projected gradient ascent and the amortization gap. Therefore, in this paper, we propose to replace Max-SW with distributional sliced Wasserstein distance with von Mises-Fisher (vMF) projecting distribution (v-DSW). Since v-DSW is a metric with any non-degenerate vMF distribution, its amortized version can guarantee the metricity when performing amortization. Furthermore, current amortized models are not permutation invariant and symmetric, thus they are not suitable to deal with set-based data (e.g. point-clouds). To address the issue, we design amortized models based on self-attention architecture. In particular, we adopt efficient self-attention architectures to make the computation linear in the number of supports. With the two improvements, we derive self-attention amortized distributional projection optimization and show its appealing performance in point-cloud reconstruction and its downstream applications.</p> <h2 id="background">Background</h2> <h3 id="optimal-transport">Optimal Transport</h3> <p>We denote a point-cloud of $m$ points \(x_1,\ldots,x_m \in \mathbb{R}^d\) (\(d \geq 1\)) as \(X=(x_1,\ldots,x_m) \in \mathbb{R}^{dm}\) which is a vector of a concatenation of all points in the point-cloud. We denote the set of all possible point-clouds as \(\mathcal{X} \subset \mathbb{R}^{dm}\). In the point-cloud representation learning, we want to estimate \(f_\phi:\mathcal{X} \to \mathcal{Z}\) (\(\phi \in \Phi\)) jointly with a function \(g_\gamma:\mathcal{Z} \to \mathcal{X}\) (\(\gamma \in \Gamma\)) given a point-cloud dataset \(p(X)\) (distribution over set of poin-clouds \(\mathcal{X}\)) by minimizing the objective:</p> \[\begin{equation} \min_{\phi \in \Phi,\gamma \in \Gamma}\mathbb{E}_{X \sim p(X)} \mathcal{D}(X,g_\gamma (f_\phi(X))). \end{equation}\] <p>Here, \(\mathcal{D}\) is a metric between two point-clouds.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/pc_reconstruction.PNG-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/pc_reconstruction.PNG-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/pc_reconstruction.PNG-1400.webp"/> <img src="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/pc_reconstruction.PNG" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 1. Overview of Point-cloud Reconstruction.</p> <h3 id="max-sliced-wasserstein-distance">Max Sliced Wasserstein Distance</h3> <p>Max sliced Wasserstein (Max-SW) distance between \(\mu \in \mathcal{P}_p(\mathbb{R}^d)\) and \(\nu\in \mathcal{P}_p(\mathbb{R}^d)\) is:</p> \[\begin{equation} \text{Max-SW}_p(\mu,\nu) = \max_{\theta \in \mathbb{S}^{d - 1}} W_p(\theta\sharp \mu,\theta \sharp \nu), \end{equation}\] <p>where the Wasserstein distance has a closed form on one dimension which is</p> \[\begin{equation} W_p(\mu,\nu) = \left( \int_0^1 |F_\mu^{-1}(z) - F_{\nu}^{-1}(z)|^{p} dz \right)^{1/p}, \end{equation}\] <p>with \(F^{-1}_{\mu}\) and \(F^{-1}_{\nu}\) are the inverse CDF of \(\mu\) and \(\nu\) respectively.</p> <p><strong>Max sliced point-cloud reconstruction:</strong> Instead of solving all optimization problems independently, an amortized model is trained to predict optimal solutions to all problems. Given a parametric function \(a_\psi: \mathcal{X}\times \mathcal{X} \to \mathbb{S}^{d-1}\) (\(\psi \in \Psi\)), the amortized objective is:</p> \[\begin{equation} \min_{\phi \in \Phi,\gamma \in \Gamma }\mathbb{E} \left[\max_{\theta \in \mathbb{S}^{d-1}}W_p(\theta \sharp P_X,\theta \sharp P_{g_\gamma (f_\phi(X))})\right], \end{equation}\] <p>the one-dimensional Wasserstein between two projected point-clouds can be solved with the time complexity \(\mathcal{O}(m\log m)\).</p> <h3 id="amortized-projection-optimization">Amortized Projection Optimization</h3> <p>Instead of solving all optimization problems independently, an amortized model is trained to predict optimal solutions to all problems. Given a parametric function \(a_\psi: \mathcal{X}\times \mathcal{X} \to \mathbb{S}^{d-1}\) (\(\psi \in \Psi\)), the amortized objective is:</p> \[\begin{equation} \min_{\phi \in \Phi,\gamma \in \Gamma}\max_{ \psi \in \Psi}\mathbb{E}_{X \sim p(X)}[W_p(\theta_{\psi,\gamma,\phi}\sharp P_X,\theta_{\psi,\gamma,\phi} \sharp P_{g_\gamma (f_\phi(X))})], \end{equation}\] <p>where \(\theta_{\psi,\gamma,\phi} = a_\psi(X,g_\gamma (f_\phi(X)))\).</p> <h2 id="self-attention-amortized-distributional-projection-optimization">Self-Attention Amortized Distributional Projection Optimization</h2> <h3 id="amortized-distributional-projection-optimization">Amortized Distributional Projection Optimization</h3> <p>Amortized optimization often leads to sub-optimality. Hence, it loses the metricity property since the Max-SW only obtains the identity of indiscernibles at the global optimum. Therefore, we propose to predict an entire distribution over projecting directions.</p> \[\begin{equation} \min_{\phi \in \Phi,\gamma \in \Gamma}\max_{ \psi \in \Psi}\mathbb{E}_{X \sim p(X)} \Big(\mathbb{E}_{\theta \sim \text{vMF}(\epsilon_{\psi,\gamma,\phi},\kappa)} W_p^p(\theta \sharp P_X,\theta \sharp P_{g_\gamma (f_\phi(X))})\Big)^{\frac{1}{p}}, \end{equation}\] <p>where \(\epsilon_{\psi,\gamma,\phi} = a_\psi(X,g_\gamma (f_\phi(X)))\), \(\text{vMF}(\epsilon,\kappa)\) is the von Mises Fisher distribution with the mean location parameter \(\epsilon \in \mathbb{S}^{d-1}\) and the concentration parameter \(\kappa &gt; 0\), and</p> \[\begin{equation} \text{v-DSW}_p(\mu,\nu;\kappa) =\max_{\epsilon \in \mathbb{S}^{d-1}} \Big(\mathbb{E}_{\theta \sim \text{vMF}(\epsilon,\kappa)} \text{W}_p^p(\theta \sharp \mu,\theta \sharp \nu) \Big)^{\frac{1}{p}} \end{equation}\] <p>is the von Mises-Fisher distributional sliced Wasserstein distance.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/amsw_avsw.PNG-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/amsw_avsw.PNG-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/amsw_avsw.PNG-1400.webp"/> <img src="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/amsw_avsw.PNG" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 2. The difference between amortized projection optimization and amortized distributional projection optimization.</p> <h3 id="self-attention-amortized-models">Self-Attention Amortized Models</h3> <p>Based on the self-attention mechanism, we introduce the self-attention amortized model which is permutation invariant and symmetric. Given \(X,Y \in \mathbb{R}^{dm}\), the <em>self-attention amortized model</em> is defined as:</p> \[\begin{equation} a_\psi (X,Y)=\frac{\mathcal{A}_{\zeta}(X'^\top)^\top \boldsymbol{1}_{m} + \mathcal{A}_{\zeta}(Y'^\top)^\top \boldsymbol{1}_{m}}{||\mathcal{A}_{\zeta}(X'^\top)^\top \boldsymbol{1}_{m} + \mathcal{A}_{\zeta}(Y'^\top)^\top \boldsymbol{1}_{m}||_2}, \end{equation}\] <p>where \(X'\) and \(Y'\) are matrices of size \(d\times m\) that are reshaped from the concatenated vectors \(X\) and \(Y\) of size \(dm\), \(\boldsymbol{1}_{m}\) is the $m$-dimensional vector whose all entries are \(1\), and \(\mathcal{A}_{\zeta}(\cdot)\) is linear (efficient) attention module [5, 6] for preserving near-linear complexity.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/amortized_models.PNG-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/amortized_models.PNG-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/amortized_models.PNG-1400.webp"/> <img src="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/amortized_models.PNG" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 3. Visualization of an amortized model that is not symmetric and permutation invariant in two dimensions.</p> <h2 id="experiments">Experiments</h2> <p>To verify the effectiveness of our proposal, we evaluate our methods on the point-cloud reconstruction task and its two downstream tasks including transfer learning and point-cloud generation (please see our papers for more details).</p> <p style="text-align:center; font-style: italic"><img src="/assets/img/reconstruction_quantitative.PNG" alt="reconstruction_quantitative" style="display:block; margin-left:auto; margin-right:auto"/> Table 1. Reconstruction and transfer learning performance on the ModelNet40 dataset. CD and SW are multiplied by 100.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/reconstruction_qualitative-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/reconstruction_qualitative-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/reconstruction_qualitative-1400.webp"/> <img src="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/reconstruction_qualitative.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 4. Qualitative results of reconstructing point-clouds in the ShapeNet Core-55 dataset. From top to bottom, the point-clouds are input, SW, Max-SW, v-DSW, and $\mathcal{L}\mathcal{A}$v-DSW respectively.</p> <h2 id="conclusion">Conclusion</h2> <p>In this paper, we have proposed a self-attention amortized distributional projection optimization framework which uses a self-attention amortized model to predict the best discriminative distribution over projecting direction for each pair of probability measures. The efficient self-attention mechanism helps to inject the geometric inductive biases which are permutation invariance and symmetry into the amortized model while remaining fast computation. Furthermore, the amortized distribution projection optimization framework guarantees the metricity for all pairs of probability measures while the amortization gap still exists. On the experimental side, we compare the new proposed framework to the conventional amortized projection optimization framework and other widely-used distances in the point-cloud reconstruction application and its two downstream tasks including transfer learning and point-cloud generation to show the superior performance of the proposed framework. For further information, please refer to our work at <a href="https://proceedings.mlr.press/v202/nguyen23e/nguyen23e.pdf">https://proceedings.mlr.press/v202/nguyen23e/nguyen23e.pdf</a>.</p> <h2 id="references">References</h2> <p>[1] Nguyen, T., Pham, Q.-H., Le, T., Pham, T., Ho, N., and Hua,B.-S. Point-set distances for learning representations of 3d point clouds. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.</p> <p>[2] Naderializadeh, N., Comer, J., Andrews, R., Hoffmann, H., and Kolouri, S. Pooling by sliced-Wasserstein embedding. Advances in Neural Information Processing Systems, 34, 2021.</p> <p>[3] Deshpande, I., Hu, Y.-T., Sun, R., Pyrros, A., Siddiqui, N., Koyejo, S., Zhao, Z., Forsyth, D., and Schwing, A. G. Max-sliced Wasserstein distance and its use for GANs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 10648–10656, 2019.</p> <p>[4] Nguyen, K. and Ho, N. Amortized projection optimization for sliced Wasserstein generative models. Advances in Neural Information Processing Systems, 2022.</p> <p>[5] Shen, Z., Zhang, M., Zhao, H., Yi, S., and Li, H. Efficient attention: Attention with linear complexities. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 3531–3539, 2021.</p> <p>[6] Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.</p>]]></content><author><name></name></author><category term="conference"/><category term="optimal-transport"/><category term="sliced-wasserstein"/><category term="point-cloud"/><category term="self-attention"/><summary type="html"><![CDATA[Table of contents Introduction Background Optimal Transport Max Sliced Wasserstein Distance Amortized Projection Optimization Self-Attention Amortized Distributional Projection Optimization Amortized Distributional Projection Optimization Self-Attention Amortized Models Experiments Conclusion References]]></summary></entry><entry><title type="html">On Transportation of Mini-batches: A Hierarchical Approach</title><link href="https://nguyendangson.github.io/https://github.com/nguyendangson/nguyendangson.github.io/blog/2022/BoMbOT/" rel="alternate" type="text/html" title="On Transportation of Mini-batches: A Hierarchical Approach"/><published>2022-08-26T00:00:00+00:00</published><updated>2022-08-26T00:00:00+00:00</updated><id>https://nguyendangson.github.io/https://github.com/nguyendangson/nguyendangson.github.io/blog/2022/BoMbOT</id><content type="html" xml:base="https://nguyendangson.github.io/https://github.com/nguyendangson/nguyendangson.github.io/blog/2022/BoMbOT/"><![CDATA[<p><strong>Table of contents</strong></p> <ul> <li><a href="#introduction">Introduction</a></li> <li><a href="#background">Background</a> <ul> <li><a href="#optimal-transport">Optimal Transport</a></li> <li><a href="#mini-batch-optimal-transport">Mini-batch Optimal Transport</a></li> </ul> </li> <li><a href="#batch-of-mini-batches-optimal-transport">Batch of Mini-batches Optimal Transport</a></li> <li><a href="#experiments">Experiments</a></li> <li><a href="#conclusion">Conclusion</a></li> <li><a href="#references">References</a></li> </ul> <h2 id="introduction">Introduction</h2> <p>The Optimal Transport (OT) theory has a long history in Applied mathematics and economics, and recently it has become a useful tool in machine learning applications such as deep generative models [1], domain adaptation [2], etc. Despite its popularity in ML, there are still major issues with using OT in large-scale datasets, those issues could be demonstrated in two following situations: “<strong>What if the number of supports is very large, for example millions?</strong>” and “<strong>What if the computation of optimal transport is repeated multiple times and has limited memory e.g., in deep learning?</strong>”. To deal with those problems, practitioners often replace the original large-scale computation of OT with cheaper computation on subsets of the whole dataset, which is widely referred to as mini-batch approaches [3, 4]. In particular, a min-batch is a sparse representation of the data. Despite being applied successfully, the current mini-batch OT loss does not consider the relationship between mini-batches and treats every pair of mini-batches the same. This causes undesirable effects in measuring the discrepancy between probability measures. First, the m-OT loss is shown to be an approximation of a discrepancy (the population m-OT) that does not preserve the metricity property, namely, this discrepancy is always positive even when two probability measures are identical. Second, it is also unclear whether this discrepancy achieves the minimum value when the two probability measures are the same. That naturally raises the question of whether we could propose a better mini-batch scheme to sort out these issues to improve the performance of the OT in applications.</p> <h2 id="background">Background</h2> <h3 id="optimal-transport">Optimal Transport</h3> <p>Let \(\mu, \nu\) be discrete distributions of $n$ supports, i.e. \(\mu := \frac{1}{n} \sum_{i=1}^n \delta_{x_i}\) and \(\nu := \frac{1}{n}\sum_{j=1}^{n} \delta_{y_j}\). Given distances between supports of two distributions as a matrix \(C\), the Optimal Transport (OT) problem reads:</p> \[\begin{equation} \text{OT}(\mu, \nu) = \min_{\pi \in \Pi(\mu, \nu)} \langle C,\pi \rangle \end{equation}\] <p>where \(\Pi(\mu, \nu) = \{ \pi \in \mathbb{R}_{+}^{n \times n} \mid \pi 1 = \mu, \pi^T 1 = \nu \}\) is the set of admissible transportation plans between \(\mu\) and \(\nu\).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/ot_example-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/ot_example-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/ot_example-1400.webp"/> <img src="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/ot_example.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 1. An example of OT with \(n = 4\).</p> <h3 id="mini-batch-optimal-transport">Mini-batch Optimal Transport</h3> <p>The original \(n\) samples are divided into random mini-batches of size \(m \geq 1\), then an alternative solution to the original OT problem is formed by averaging these smaller OT solutions.</p> \[\begin{equation} \text{m-OT}^m(\mu, \nu) = \mathbb{E}_{(X, Y) \sim \overset{\otimes m}{\mu} \otimes \overset{\otimes m}{\nu}} [\text{OT}(P_X, P_Y)] \end{equation}\] <p>where \(\otimes\) denotes product measure, \(X = (x_1, \ldots, x_m)\) is the sampled mini-batch, and \(P_X = \frac{1}{m} \sum_{i=1}^m \delta_{x_i}\) is the corresponding discrete distribution. In practice, we can use subsampling to approximate the expectation, thus the empirical m-OT reads:</p> \[\begin{equation} \text{m-OT}_k^m(\mu, \nu) \approx \frac{1}{k} \sum_{i=1}^k [\text{OT}(P_{X_i}, P_{Y_i})] \end{equation}\] <p>where \((X_1, Y_1), \ldots, (X_k, Y_k) \sim \overset{\otimes m}{\mu} \otimes \overset{\otimes m}{\nu}\) and \(k\) is often set to 1 in previous works.</p> <p><strong>Issue of m-OT:</strong> We can see that the optimal matchings at the mini-batch level in Figure 2 are different from the full-scale optimal transport. One source of the issue is that all pairs of mini-batches are treated the same.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mot_example2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mot_example2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mot_example2-1400.webp"/> <img src="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mot_example2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 2. An example of m-OT with \(n = 4, m = 2\) and \(k = 2\).</p> <h2 id="batch-of-mini-batches-optimal-transport">Batch of Mini-batches Optimal Transport</h2> <p>To address the issues of m-OT, we solve an additional OT problem between mini-batches to find an optimal weighting for combining local mini-batch losses.</p> \[\begin{equation} \text{BoMb-OT}^m(\mu, \nu) = \inf_{\gamma \in \Gamma(\overset{\otimes m}{\mu} \otimes \overset{\otimes m}{\nu})} \mathbb{E}_{(X, Y) \sim \gamma} [\text{OT}(P_X, P_Y)] \end{equation}\] <p>where \(\otimes\) denotes product measure, \(X = (x_1, \ldots, x_m)\) is the sampled mini-batch, and \(P_X = \frac{1}{m} \sum_{i=1}^m \delta_{x_i}\) is the corresponding discrete distribution. In practice, we can use subsampling to approximate the expectation, thus the empirical BoMb-OT reads:</p> \[\begin{equation} \text{BoMb-OT}_k^m(\mu, \nu) \approx \inf_{\gamma \in \Gamma(\overset{\otimes m}{\mu_k} \otimes \overset{\otimes m}{\nu_k})} \sum_{i=1}^k \sum_{j=1}^k \gamma_{ij}[\text{OT}(P_{X_i}, P_{Y_j})] \end{equation}\] <p>where \(X_1, \ldots, X_k \sim \overset{\otimes m}{\mu}\) and \(\overset{\otimes m}{\mu_k} = \frac{1}{k} \sum_{i=1}^k \delta_{X_i}\). \(Y_j (1 \leq j \leq k)\) and \(\overset{\otimes m}{\nu_k}\) are defined similarly.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/bombot_example-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/bombot_example-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/bombot_example-1400.webp"/> <img src="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/bombot_example.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 3. An example of BoMb-OT with \(n = 4, m = 2\) and \(k = 2\). After solving the OT problem between mini-batches, \(X_1\) is mapped to \(Y_2\) and \(X_2\) is mapped to \(Y_1\), which results in the same solution as the full-scale optimal transport.</p> <p><strong>Training deep networks with BoMb-OT loss:</strong> In the deep learning context, the supports are usually parameterized by neural networks. In addition, the gradient of neural networks is accumulated from each pair of mini-batches and only one pair of mini-batches are used in memory at a time. Since the computations on pairs of mini-batches are independent, we can use multiple devices to compute them. We propose a three-step algorithm to train neural networks with BoMb-OT loss as follows.</p> <p><img src="/assets/img/training_bombot_loss.png" alt="training_bombot_loss" style="display:block; margin-left:auto; margin-right:auto"/></p> <h2 id="experiments">Experiments</h2> <p>BoMb-(U)OT shows a favorable performance compared to m-(U)OT on three types of applications, namely, <em>gradient-based</em> (e.g., deep generative model, deep domain adaptation (DA)), <em>mapping-based</em> (e.g., color transfer), and <em>value-based</em> (e.g., approximate Bayesian computation (ABC)).</p> <p style="text-align:center; font-style: italic"><img src="/assets/img/generative_model.png" alt="generative_model" style="display:block; margin-left:auto; margin-right:auto"/> Table 1. Comparison between the BoMb-OT and the m-OT on deep generative models. On the MNIST dataset, we evaluate the performances of generators by computing approximated Wasserstein-2 while we use the FID score on CIFAR10 and CelebA.</p> <p style="text-align:center; font-style: italic"><img src="/assets/img/BoMbOT_DA_VisDA.png" alt="BoMbOT_DA_VisDA" style="display:block; margin-left:auto; margin-right:auto"/> Table 2. Comparison between two mini-batch schemes on the deep domain adaptation on the VisDA dataset. We varied the number of mini-batches k and reported the classification accuracy on the target domain.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/color_transfer-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/color_transfer-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/color_transfer-1400.webp"/> <img src="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/color_transfer.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 4. Experimental results on color transfer for full OT, the m-OT, and the BoMb-OT on natural images with \((k; m) = (10; 10)\). Color palettes are shown under corresponding images.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/ABC-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/ABC-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/ABC-1400.webp"/> <img src="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/ABC.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 5. Approximated posteriors from ABC with the m-OT and the BoMb-OT. The first row, the second row, and the last row have \(m = 8, m = 16\), and \(m = 32\), respectively. In each row, the number of mini-batches k is 2; 4; 6; and 8 from left to right.</p> <h2 id="conclusion">Conclusion</h2> <p>In this paper, we have presented a novel mini-batch method for optimal transport, named Batch of Mini-batches Optimal Transport (BoMb-OT). The idea of the BoMb-OT is to consider the optimal transport problem on the space of mini-batches with an OT-types ground metric. More importantly, we have shown that the BoMb-OT can be implemented efficiently and they have more favorable performance than the m-OT in various applications of optimal transport including deep generative models, deep domain adaptation, color transfer, approximate Bayesian computation, and gradient flow. For future work, we could consider a hierarchical approach version of optimal transport between incomparable spaces. For further information, please refer to our work at <a href="https://proceedings.mlr.press/v162/nguyen22d/nguyen22d.pdf">https://proceedings.mlr.press/v162/nguyen22d/nguyen22d.pdf</a>.</p> <h2 id="references">References</h2> <p>[1] Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein generative adversarial networks. In International Conference on Machine Learning, pp. 214–223, 2017.</p> <p>[2] Courty, N., Flamary, R., Tuia, D., and Rakotomamonjy, A. Optimal transport for domain adaptation. IEEE transactions on pattern analysis and machine intelligence, 39(9):1853–1865, 2016.</p> <p>[3] Fatras, K., Zine, Y., Flamary, R., Gribonval, R., and Courty, N. Learning with minibatch Wasserstein: asymptotic and gradient properties. In AISTATS 2020-23nd International Conference on Artificial Intelligence and Statistics, volume 108, pp. 1–20, 2020.</p> <p>[4] Fatras, K., Zine, Y., Majewski, S., Flamary, R., Gribonval, R., and Courty, N. Minibatch optimal transport distances; analysis and applications. arXiv preprint arXiv:2101.01792, 2021b.</p>]]></content><author><name></name></author><category term="conference"/><category term="optimal-transport"/><category term="domain-adaptation"/><category term="generative-models"/><summary type="html"><![CDATA[Table of contents Introduction Background Optimal Transport Mini-batch Optimal Transport Batch of Mini-batches Optimal Transport Experiments Conclusion References]]></summary></entry><entry><title type="html">Improving Mini-batch Optimal Transport via Partial Transportation</title><link href="https://nguyendangson.github.io/https://github.com/nguyendangson/nguyendangson.github.io/blog/2022/mPOT/" rel="alternate" type="text/html" title="Improving Mini-batch Optimal Transport via Partial Transportation"/><published>2022-08-26T00:00:00+00:00</published><updated>2022-08-26T00:00:00+00:00</updated><id>https://nguyendangson.github.io/https://github.com/nguyendangson/nguyendangson.github.io/blog/2022/mPOT</id><content type="html" xml:base="https://nguyendangson.github.io/https://github.com/nguyendangson/nguyendangson.github.io/blog/2022/mPOT/"><![CDATA[<p><strong>Table of contents</strong></p> <ul> <li><a href="#introduction">Introduction</a></li> <li><a href="#background">Background</a> <ul> <li><a href="#optimal-transport">Optimal Transport</a></li> <li><a href="#mini-batch-optimal-transport">Mini-batch Optimal Transport</a></li> </ul> </li> <li><a href="#mini-batch-partial-optimal-transport">Mini-batch Partial Optimal Transport</a> <ul> <li><a href="#partial-optimal-transport">Partial Optimal Transport</a></li> <li><a href="#mini-batch-partial-optimal-transport-1">Mini-batch Partial Optimal Transport</a></li> <li><a href="#training-deep-networks-with-m-pot-loss">Training deep networks with m-POT loss</a></li> </ul> </li> <li><a href="#experiments">Experiments</a></li> <li><a href="#conclusion">Conclusion</a></li> <li><a href="#references">References</a></li> </ul> <h2 id="introduction">Introduction</h2> <p>The Optimal Transport (OT) theory has a long history in Applied mathematics and economics, and recently it has become a useful tool in machine learning applications such as deep generative models [1], domain adaptation [2], etc. Despite its popularity in ML, there are still major issues with using OT in large-scale datasets, those issues could be demonstrated in two following situations: “<strong>What if the number of supports is very large, for example millions?</strong>” and “<strong>What if the computation of optimal transport is repeated multiple times and has limited memory e.g., in deep learning?</strong>”. To deal with those problems, practitioners often replace the original large-scale computation of OT with cheaper computation on subsets of the whole dataset, which is widely referred to as mini-batch approaches [3, 4]. In particular, a min-batch is a sparse representation of the data. Hence, matching two sparse subsets of two datasets often leads to many wrong pairings between sample points of two distributions. That consequently results in the extremely inaccurate estimation of the transport plan and OT cost between distributions.</p> <h2 id="background">Background</h2> <h3 id="optimal-transport">Optimal Transport</h3> <p>Let \(\mu,\nu\) be discrete distributions of \(n\) supports, i.e. \(\mu := \frac{1}{n} \sum_{i=1}^n \delta_{x_i}\) and \(\nu := \frac{1}{n}\sum_{j=1}^{n} \delta_{y_j}\). Given distances between supports of two distributions as a matrix \(C\), the Optimal Transport (OT) problem reads:</p> \[\begin{equation} \text{OT}(\mu, \nu) = \min_{\pi \in \Pi(\mu, \nu)} \langle C,\pi \rangle \end{equation}\] <p>where \(\Pi(\mu, \nu) = \{ \pi \in \mathbb{R}_{+}^{n \times n} \mid \pi 1 = \mu, \pi^T 1 = \nu \}\) is the set of admissible transportation plans between \(\mu\) and \(\nu\).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/ot_example-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/ot_example-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/ot_example-1400.webp"/> <img src="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/ot_example.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 1. An example of OT with \(n = 4\).</p> <h3 id="mini-batch-optimal-transport">Mini-batch Optimal Transport</h3> <p>The original \(n\) samples are divided into random mini-batches of size \(m \geq 1\), then an alternative solution to the original OT problem is formed by averaging these smaller OT solutions.</p> \[\begin{equation} \text{m-OT}^m(\mu, \nu) = \mathbb{E}_{(X, Y) \sim \overset{\otimes m}{\mu} \times \overset{\otimes m}{\nu}} [\text{OT}(P_X, P_Y)] \end{equation}\] <p>where \(\otimes\) denotes product measure, \(X = (x_1, \ldots, x_m)\) is the sampled mini-batch, and \(P_X = \frac{1}{m} \sum_{i=1}^m \delta_{x_i}\) is the corresponding discrete distribution. In practice, we can use subsampling to approximate the expectation, thus the empirical m-OT reads:</p> \[\begin{equation} \text{m-OT}_k^m(\mu, \nu) \approx \frac{1}{k} \sum_{i=1}^k [\text{OT}(P_{X_i}, P_{Y_i})] \end{equation}\] <p>where \((X_1, Y_1), \ldots, (X_k, Y_k) \sim \overset{\otimes m}{\mu} \otimes \overset{\otimes m}{\nu}\) and \(k\) is often set to 1 in previous works.</p> <p><strong>Misspecified matchings issue of m-OT:</strong> We can see that the optimal matchings at the mini-batch level in Figure 2 are different from the full-scale optimal transport. We call these pairs misspecified matchings since they are optimal on the local mini-batch scale but they are non-optimal on the global scale. The reason is that all samples in mini-batches are forced to be transported.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mot_example-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mot_example-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mot_example-1400.webp"/> <img src="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mot_example.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 2. An example of m-OT with \(n = 4, m = 2\) and \(k = 2\).</p> <h2 id="mini-batch-partial-optimal-transport">Mini-batch Partial Optimal Transport</h2> <p>To alleviate misspecified matchings, we use partial optimal transport between mini-batches levels instead of optimal transport. The partial optimal transport is defined almost the same as optimal transport except it only allows a fraction of masses $s$ to be transported.</p> <h3 id="partial-optimal-transport">Partial Optimal Transport</h3> <p>Let \(\mu, \nu\) be discrete distributions of \(n\) supports. Given the fraction of masses \(0 \leq s \leq 1\), the Partial Optimal Transport (POT) problem reads:</p> \[\begin{equation} \text{POT}^s(\mu, \nu) = \min_{\pi \in \Pi_s(\mu, \nu)} \langle C,\pi \rangle \end{equation}\] <p>where \(\Pi_s(\mu, \nu) = \{ \pi \in \mathbb{R}_{+}^{n \times n} \mid \pi 1 \leq \mu, \pi^T 1 \leq \nu, 1^T \pi 1 = s \}\) is the set of admissible transportation plans between \(\mu\) and \(\nu\).</p> <h3 id="mini-batch-partial-optimal-transport-1">Mini-batch Partial Optimal Transport</h3> <p>Similar to m-OT, we define mini-batch POT (m-POT) which averages the partial optimal transport between mini-batches of size \(m \geq 1\) as:</p> \[\begin{equation} \text{m-POT}^{m,s}(\mu, \nu) = \mathbb{E}_{(X, Y) \sim \overset{\otimes m}{\mu} \otimes \overset{\otimes m}{\nu}} [\text{POT}^s(P_X, P_Y)] \end{equation}\] <p>where \(\otimes\) denotes product measure, \(X = (x_1, \ldots, x_m)\) is the sampled mini-batch, and \(P_X = \frac{1}{m} \sum_{i=1}^m \delta_{x_i}\) is the corresponding discrete distribution. In practice, we can use subsampling to approximate the expectation, thus the empirical m-POT reads:</p> \[\begin{equation} \text{m-POT}_k^{m, s}(\mu, \nu) \approx \frac{1}{k} \sum_{i=1}^k [\text{POT}^s(P_{X_i}, P_{Y_i})] \end{equation}\] <p>where \((X_1, Y_1), \ldots, (X_k, Y_k) \sim \overset{\otimes m}{\mu} \otimes \overset{\otimes m}{\nu}\) and \(k\) is often set to 1.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mpot_example-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mpot_example-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mpot_example-1400.webp"/> <img src="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mpot_example.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 3. An example of m-POT with \(n = 4, m = 2, k = 2\) and \(s = \frac{1}{2}\).</p> <p>In Figure 3, POT gives the exact 2 matchings, alleviating the misspecified matchings issue.</p> <h3 id="training-deep-networks-with-m-pot-loss">Training deep networks with m-POT loss</h3> <p><strong>Parallel training:</strong> In the deep learning context, the supports are usually parameterized by neural networks. In addition, the gradient of neural networks is accumulated from each pair of mini-batches and only one pair of mini-batches are used in memory at a time. Since the computations on pairs of mini-batches are independent, we can use multiple devices to compute them.</p> <p><strong>Two-stage training:</strong> We also propose two-stage training for <em>domain adaptation</em>. We first find matchings between pairs of bigger mini-batches of size \(M\) on RAM. Then use it to obtain a mapping to create smaller mini-batches of size $m$ which are used on GPU for estimating the gradient of neural networks. This algorithm allows us to have better matchings since they are obtained from larger transportation problems.</p> <h2 id="experiments">Experiments</h2> <p>To validate the performance of the proposed methods, we carry out experiments on deep domain adaptation (DA). We observe that m-POT gives better-adapted classification accuracy on all datasets than the previous methods. Moreover, the two-stage training significantly improves the performance of DA on Office-Home and VisDA for both optimal transport and partial optimal transport.</p> <p style="text-align:center; font-style: italic"><img src="/assets/img/mPOT_DA_digits.png" alt="mPOT_DA_digits" style="display:block; margin-left:auto; margin-right:auto"/> Table 1. DA results in classification accuracy on digits datasets (higher is better).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mPOT_DA_OfficeHome-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mPOT_DA_OfficeHome-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mPOT_DA_OfficeHome-1400.webp"/> <img src="/https://github.com/nguyendangson/nguyendangson.github.io/assets/img/mPOT_DA_OfficeHome.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p style="text-align:center; font-style: italic">Table 2. DA results in classification accuracy on the Office-Home dataset (higher is better).</p> <p style="text-align:center; font-style: italic"><img src="/assets/img/mPOT_DA_VisDA.png" alt="mPOT_DA_VisDA" style="display:block; margin-left:auto; margin-right:auto"/> Table 3. DA results in classification accuracy on the VisDA dataset (higher is better).</p> <h2 id="conclusion">Conclusion</h2> <p>In this paper, we have introduced a novel mini-batch approach that is referred to as mini-batch partial optimal transport (m-POT). The new mini-batch approach is motivated by the issue of misspecified mappings in the conventional mini-batch optimal transport approach (m-OT). Via extensive experiment studies, we demonstrate that m-POT can perform better than current mini-batch methods including m-OT and m-UOT in domain adaptation applications. Furthermore, we propose the two-stage training approach for the deep DA that outperforms the conventional implementation. For further information, please refer to our work at <a href="https://proceedings.mlr.press/v162/nguyen22e/nguyen22e.pdf">https://proceedings.mlr.press/v162/nguyen22e/nguyen22e.pdf</a>.</p> <h2 id="references">References</h2> <p>[1] Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein generative adversarial networks. In International Conference on Machine Learning, pp. 214–223, 2017.</p> <p>[2] Courty, N., Flamary, R., Tuia, D., and Rakotomamonjy, A. Optimal transport for domain adaptation. IEEE transactions on pattern analysis and machine intelligence, 39(9):1853–1865, 2016.</p> <p>[3] Fatras, K., Zine, Y., Flamary, R., Gribonval, R., and Courty, N. Learning with minibatch Wasserstein: asymptotic and gradient properties. In AISTATS 2020-23nd International Conference on Artificial Intelligence and Statistics, volume 108, pp. 1–20, 2020.</p> <p>[4] Fatras, K., Zine, Y., Majewski, S., Flamary, R., Gribonval, R., and Courty, N. Minibatch optimal transport distances; analysis and applications. arXiv preprint arXiv:2101.01792, 2021b.</p>]]></content><author><name></name></author><category term="conference"/><category term="optimal-transport"/><category term="domain-adaptation"/><category term="generative-models"/><summary type="html"><![CDATA[Table of contents Introduction Background Optimal Transport Mini-batch Optimal Transport Mini-batch Partial Optimal Transport Partial Optimal Transport Mini-batch Partial Optimal Transport Training deep networks with m-POT loss Experiments Conclusion References]]></summary></entry></feed>